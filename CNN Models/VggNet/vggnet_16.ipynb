{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  VGGNet\n",
    "VGGNet is the 1st runner-up, not the winner of the ILSVRC-2014 in the classification task. VGGNet beats the GoogLeNet and won the localization task in ILSVRC 2014\n",
    "\n",
    "### The highlights of the paper\n",
    "- The Use of 3×3 Filters instead of large-size filters (such as 11×11, 7×7).\n",
    "- Multi-Scale Training & Testing\n",
    "- Dense (Convolutionalized) Testing\n",
    "- Model Fusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architecture\n",
    "During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers. All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity. The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512. Number of layers depend on VGG-11, VGG-11 (LRN), VGG-13, VGG-16 (Conv1), VGG-16 and VGG-19 architechure.\n",
    "![orignal_architeture](./images/original_architechure.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGGNet implementation on Cifar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 small image classification\n",
    "Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n",
    "\n",
    "#### Returns 2 tuples:\n",
    "- **x_train, x_test**: uint8 array of RGB image data with shape (num_samples, 32, 32, 3).\n",
    "- **y_train, y_test**: uint8 array of category labels (integers in range 0-9) with shape (num_samples,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traning on Gpu\n"
     ]
    }
   ],
   "source": [
    "is_gpu_available = torch.cuda.is_available()\n",
    "if is_gpu_available:\n",
    "    print(\"Traning on Gpu\")\n",
    "else:\n",
    "    print(\"Traning on Cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples = 50000\n",
      "Number of testing examples = 10000\n",
      "Image data shape = torch.Size([3, 32, 32])\n",
      "Number of classes = 10\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, ToTensor, RandomRotation, Normalize\n",
    "\n",
    "transform = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(10),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_data = CIFAR10('data', transform=transform, train=True, download=True)\n",
    "test_data = CIFAR10('data', transform=transform, train=False, download=True)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "print(\"Number of training examples =\", len(train_data))\n",
    "print(\"Number of testing examples =\", len(test_data))\n",
    "print(\"Image data shape =\", train_data[0][0].shape)\n",
    "print(\"Number of classes =\", len(classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(0.2 * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data, batch_size=20, sampler=train_sampler)\n",
    "valid_loader = DataLoader(dataset=train_data, batch_size=20, sampler=valid_sampler)\n",
    "test_loader = DataLoader(dataset=test_data, batch_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize a Batch of Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAHelJREFUeJztnWmMnNeVnt9TSy/sbnY3d1KkREqiFo5Wh5HtkcawPRpHNgaRDCSCjcSjH87QCGwkBpwfggPEDpAfniC24V8O6EgYzYzHS2wZVgbKjBTFjkaWTIuSKIoiRYmiKe7sJrubvdb21cmPKiUtzn1vF3uppnzfByBYfU/d+k7d+k59Vfetc465O4QQ6ZFbbgeEEMuDgl+IRFHwC5EoCn4hEkXBL0SiKPiFSBQFvxCJouAXIlEU/EIkSmEhk83sPgDfAZAH8N/c/Rtz3F8/J1wEbrr1zojVgqM51Od5tPDjxS1Arc5ean4KxB4vxmKfVO6x58yP5tFnwOcV8uFr8MHXXok8XuRIsScwC5vvz3vNLA/gTQB/BOAkgBcBfNbdD0bmKPgXgReOT1Jbjryfr6jP0Dme4+dKzorUVoicO+dmsuB43mv8WAjPAQBYnppipzA7v834cy7XuC0fCeJa7E0jcuqv6e8Kjt+2pYfOidFq8C/kY/9dAI64+1F3rwD4IYD7F/B4Qog2spDgvwrAiVl/n2yOCSHeByzoO38rmNkuALuW+jhCiMtjIcF/CsCWWX9vbo69B3ffDWA3oO/8QlxJLORj/4sAtpvZNjPrAPAZAE8sjltCiKVm3ld+d6+Z2ZcA/B0aUt+j7v76onm2hBw4cZHasjrfVUY9vIla9BKfEpHYKpGddM/xne+Z0lk+LxsIjpe9SueUC9zHGlZQmzk/fUoefm4F59eboleozY3Py2Kb20wKiE7h61HPuM1y/NyJiAuLr1W2yIK+87v7kwCeXCRfhBBtRL/wEyJRFPxCJIqCX4hEUfALkSgKfiESZcl/4TebW2+7E0889WzQVskiCRPkPcpjGkkk22N0kktK1TqXcjoRTo4ZKJ/kblQmqC3rXk1t9S4uGxUPPkVtnX3hJBEr9tM5tpJnCU53camvTC1Awdg6RqRUj9giRFW0eaQKxhKM8jEZMHLuxM7HXL2tYfj/j7ssRxVCLDsKfiESRcEvRKIo+IVIFAW/EInS1m1Gh9Md0Ui+BKYr4dJP1RovCRWrp1YjSScAkM9PUVvn1GvB8YsH/yedM3OKVjVDrm8tta1cewO1ZROj1DZ28lDYj6lpOqd30yepre+Wf0ptue511JaRNa5FkoFsnjXwLJI8ZXSXPVJTr8bX16qR5KPI+RgrG5YVuKKylOjKL0SiKPiFSBQFvxCJouAXIlEU/EIkioJfiERpr9TnjkolLJV0dPfSeVYIuzkxxWvnlSKySy7S/aW78g8KEP8/aifDCTUjx56nc1bUuI95cB+R54k40xtu49M8nG7TN7GPzpk6+ji1ZZ1cFl15M+/RUs6H55XykZqAGT8HvMaTbQxcfitauHahReoFonyOmuoV7kcuIlVOT3Op1cvhZKylRld+IRJFwS9Eoij4hUgUBb8QiaLgFyJRFPxCJMqCpD4zOwZgAkAGoObuO2P3r9frmJqaDNo6ajzLKl/oDI4XI5lStUh7p2g24Pm3qW1lOSwDrriR18DzAu9abt2bqa2UCz9nABi6GK4lCABdfmNwfO0mnkHoE7zL2sgbP6O2znzk9Ln5geBw3+QYnTJWjFQFzPhrtiLHz4OOPGlTFkkgzCJZgrk89yPL5hdOFy5cCI7/6rlwvUsAuPuej8zrWLNZDJ3/Y+5+fhEeRwjRRvSxX4hEWWjwO4CnzOwlM9u1GA4JIdrDQj/23+Pup8xsHYCnzewNd3/PF5Xmm8IuANiwcdMCDyeEWCwWdOV391PN/4cA/AzAXYH77Hb3ne6+c3AVb1IhhGgv8w5+M+sxs753bwP4BIADi+WYEGJpWcjH/vUAftYsTFgA8Nfu/rexCV53VCph6SWr8bZWOYSLatatg86pR58a13m8yqWcmYmwJLN2O5fsat03Udv58vw+CU2e4dLcBZI9drraQ+f0dn+Q2vq3cz9mRo9SW/3vvxp+vI5VdM7gIF/HymbuY6GwntryJIOzVuVZdrmMn1cekaQnp3jx15i83NUVzupj44vFvIPf3Y8CuH0RfRFCtBFJfUIkioJfiERR8AuRKAp+IRJFwS9EorS1gGeWZbh4MSzpdfXwLLaOYtjNAu3DBuScZHMB8EhGV3XDx6jNiO51+OAjdM5g/i+p7epr/5DaXjnPC3jmMv4E8ugOjpctnE0JAOcneJHR8RK/PvSu3EJtm3rCWYTT9UhW3MUj/FhF/lqvu+bj1DZeCp875VLk/Kjx5zw+xjMqa5GCrF3dXLbL5cLHy+d5odnFQFd+IRJFwS9Eoij4hUgUBb8QiaLgFyJR2rrbX3dHpRxuk1QjbaYAoFgM73p2F1fSOUbaRQG8/RcAZFW+8z1d2BAcr6y8h8459vpfUdvK7t9S22D/DmrrrvFWU2+OhW25At8t9zzfia5VuLIwOh5peVUKJ8eUu3i7rnW5sFIBAPmTb1DbZJlfwzrXhFub1SLt0E5PDHE/OnnST29HpN1YRJlitnqd1xJcDHTlFyJRFPxCJIqCX4hEUfALkSgKfiESRcEvRKK0VerzeoZqaTxoyzt/H8ojnPRTcZ5kkS9kl/14AJBlPOGjXA7XfesY2ErnnFrBJbu/P3iY2u6+p4/aOge5JHZ2OOzjWI1LVIUil6hyOS5RWYVLfdNEup2Z5hIsPxKA/EbuxwleN7a/HvaxuupmOqevg58D6ObyskfafOUr/PzOSD3BWpWf34uBrvxCJIqCX4hEUfALkSgKfiESRcEvRKIo+IVIlDmlPjN7FMAfAxhy91uaY6sA/AjAVgDHADzo7qNzPVZnRwHXbFkTtI1P8RpzZ8+Gs6x6e7mc19PLJZlcpDTaoYNcNurOh4+XM575NtFzPbVdjLS72npkH7X1r76O2n5ve7gF2AsHuHw1PcMzKnu7uABXc97yqlQJS4vFOm9pVSvwF+Z8jrf5KmOM2upnXw2OF6Z45t7KXn6sWg9vezadRbIjZ/hzy6rh9S9XLtI5i0ErV/4/B3DfJWMPA3jG3bcDeKb5txDifcScwe/uzwIYuWT4fgCPNW8/BuCBRfZLCLHEzPc7/3p3P9O8fRaNjr1CiPcRC97w80YZEvrF0Mx2mdleM9s7OjrntoAQok3MN/jPmdlGAGj+T3dP3H23u+90952Dg4PzPJwQYrGZb/A/AeCh5u2HAPx8cdwRQrSLVqS+HwD4KIA1ZnYSwNcAfAPAj83s8wDeAfBgKwfr7u7ELTu2BW37Dxyi80rT4RZftSrPojp58ji1FTq5JPPWYe7HyMULwfFYluC2DWHpDQBu3sYzy15/8yVqq719itruvTc8vmETl6+OHOIy4ETGM8vqFsmcZFma+Ughy4y/ntUizwYcwtXU1pcLy7ArR7jMiojUh+EXqKl7xT+mtlonz0qc6QqfI15Y2qy+OYPf3T9LTLzRnBDiike/8BMiURT8QiSKgl+IRFHwC5EoCn4hEqWtBTxLpWm8+cb+oG18lGdmrRkcCI4fO34mOA4Ahw6/RW0ZeBbeuo3rqO3U6XD2VT7SBy8rh2VKALjjVi4N/eLMMLWNnOXZXoePh9/Pb7qZv9TnjnNJ6fR5nrnX0x/prUcuK2MzvOjniu7I45W4H7USz0o8kt8aHO/tv4nOWXMinAkIAB3Gs09XePjcBoCejTuprXPN7cHxsVL4vAeAPb95MTj+0J/8CZ1zKbryC5EoCn4hEkXBL0SiKPiFSBQFvxCJouAXIlHaKvXVajWMjoYlrEIXd2VdX7ho4tAZXhRx3VXh7MG5OHjyCLVZLpz9duLI23TOxEWeIXb2r7i8+W/+1b+ktsnTr1PbudO/DY6/sp+v7wd38GKnQyWeabf/NS57lS0sf1rkjKtFCqGaczk1n+PXsHIWlhazyUhfvY5wkVkAKJW5FLylxCVCx/PU1t0ZLmra138XnVPIwn5YRMa+FF35hUgUBb8QiaLgFyJRFPxCJIqCX4hEaetuf84K6OwI1yureYnOc7KB+ft330bnPL/nNWo7dIYnxoy89Sa3jYV3jivZOJ0zebJGbVdt3EBtY+M8WeXa626hts7u/uD4G/v20jnDY1upbd1aXu/w1hu42vL6m+E18TrfZS/znB/kC9yPPKnTBwBZLpy0tML5+l6s8ASjSNlIDBd5+4pimZ9XfuK54PjqIldhBjbsCB8n0oruUnTlFyJRFPxCJIqCX4hEUfALkSgKfiESRcEvRKK00q7rUQB/DGDI3W9pjn0dwJ8CeDdL56vu/uRcj+UGZOTtpoAVdN6ajrDMU53gusvZMzxBZ3qY2zbkeQuqoXI4uSQr8vfQ+gyXMLMS17YqZe5Hvchru225fm1wvDzK6x2+cYK3/+osbKK2G9fx+oSjJOenOsNPudNT/Dnn8lzD6jS+/oUsXPuvVOGP54UOass7l3UvFnhCUG+NJ0F1TxwMzxk9TOesWxU+9wuXId63cuX/cwD3Bca/7e53NP/NGfhCiCuLOYPf3Z8FMNIGX4QQbWQh3/m/ZGb7zexRMxtcNI+EEG1hvsH/XQDXAbgDwBkA32R3NLNdZrbXzPaOjfGf1Qoh2su8gt/dz7l75u51AN8DQEuOuPtud9/p7jsHBsK/OxdCtJ95Bb+ZbZz156cBHFgcd4QQ7aIVqe8HAD4KYI2ZnQTwNQAfNbM7ADiAYwC+0MrB8jmgvy/8fuNVXs/OS2F5pTbMWzjdsYbbNndy27Nv8dZVHfVwDb/OiNSXq3P56sIpXvvv9HEuR95++43U1kdaXq259h/ROSee/x611Se5JJZNnKC2m7ZsCY5Pj/Msx9FjfB1rxmVdd/6YBaItT3kkS5CXC0TO+Lxqmfs47BupbUUtLLUOZNwRz8LnIiK1Di9lzuB3988Ghh9p+QhCiCsS/cJPiERR8AuRKAp+IRJFwS9Eoij4hUiUthbwzFsdA8WwbHdxnP9UYGY6LAOePXmezllZI1IIgCzSFsp7+6jtnlvCy3XyNM+YG65wWbEYUWVKk/y55UjbMACwXFjqq0WKXFazTmobHTlGbQdG+Wt2L/k9V7HI/SjWuRw2Q7LzAAA5LrHlPLweWYVnJBbAs/qyyDrWapHipOR1AYBaNdx6KytEWs51kMxOa72Cp678QiSKgl+IRFHwC5EoCn4hEkXBL0SiKPiFSJT2Sn2oos/DElZPF8+mG8+FZY3/c5YXnjx1nBdMrHfxHmjWy7O2tq0Nv1cWq6N0joMX6bxhI5d/tq3k65Gv8T5ze37zQnD8wlmegbd2PS8Iev067qONDlFbvnQyOD4VkcOqpR5q80jfugxc3qpWwxl/HZGmdrmInFep8LWPMc0q1wKYrlwdHP/FXn5+z+R+GRyfGOcS5qXoyi9Eoij4hUgUBb8QiaLgFyJRFPxCJEpbd/trlRLOvxNOBjn/zut03q/fCu84P/kyb4U1U+fJKj09PHEDGW/HNH0hnFyyqY8vY28n38G+4ZpwQgcAbCfKAgCcOsJr//32lVeD42+fOErnjJ5/h9omRsO1+ABg6/pbuR9j4WSnG1afo3M+su01anvq4DXUVu/fTG3dveE2cF7ju/21WuS8irRf80j9vHqdKwjTpbAi0dPNk8zGyuHzO/PWr+e68guRKAp+IRJFwS9Eoij4hUgUBb8QiaLgFyJRWmnXtQXAXwBYj0Z7rt3u/h0zWwXgRwC2otGy60F35xkuAA68dQ7X/5NvBW37fvAv6Lxt/YeD46vqPPnlePk6aqsbr4+3uZ9LOSdnwgkwxyPP+qoe3krqV6/zWnxTRV6zbvvGX1Pb+HRYxhxYEZa8AODqW3dQ2zRfDpwf5sYyqdV3dR+X2Or5Xn6wPE+4itXOm8mIjxn3o1KdorbJSZ4wZpHakIhIcBnp6FYzLh2Ol8OSdBZpD3cprVz5awC+4u47AHwIwBfNbAeAhwE84+7bATzT/FsI8T5hzuB39zPu/nLz9gSAQwCuAnA/gMead3sMwANL5aQQYvG5rO/8ZrYVwJ0A9gBY7+7v/ozrLBpfC4QQ7xNa/nmvmfUC+CmAL7v7+OzvN+7uZuEvKGa2C8CuhToqhFhcWrrym1kRjcD/vrs/3hw+Z2Ybm/aNAIJlXdx9t7vvdPedi+GwEGJxmDP4rXGJfwTAIXefvVX/BICHmrcfAvDzxXdPCLFUtPKx/24AnwPwmpnta459FcA3APzYzD4P4B0ADy7EkXyRZzB19w4GxwcG+XvXxfNcf/u9q7lsNHKRmpCvEqPzLMELo1zOW9nPl3/f0XCLMgAYr/KMRRCpp8bVKwyuX0Vt/V18rS5GWmiNzYRr3c1kvF7g8BSX30oZryVYLXM5NUfatpkX6ZzM+WvGagICQL7Az8eICEgpV7mUjTzxIyIPXsqcwe/uz4H7/octH0kIcUWhX/gJkSgKfiESRcEvRKIo+IVIFAW/EInS1gKeMdZc/wfUVukK/3K47y1e9HOwylsd/cENXCo7d5Fnv/38hdPB8aJzOQwZl42Ked7Kq1rm4tDpES439a0IZ7gNn+dzhiq8qOb1W3jhzPFJ/pgjw2Gp9XhEnn3m5TepzQZ5xl+hm2eyeSmc1dfVxaXlWp2/LpUaP1bR+HOr1/l5kCPzunK8+GseYXnZLkNU1JVfiERR8AuRKAp+IRJFwS9Eoij4hUgUBb8QiXLFSH2nyzzb62x1bXB8phjO9gOAQsdZauspcLmmy7htgvX/K/BMqoJz6WXz6lgWGLedmJmgtnOkvuTQWT6nOsT7E/Z08Yy/1f38NauPheXPp3/N5bwDJ7jEtu0G7mN/fiX3g2Q5VjJ+rBqrqAkgq/Nioahx6TMKOUXqkYebmAi/0FkW8e8SdOUXIlEU/EIkioJfiERR8AuRKAp+IRLlitntHx7l9cpOD4UL0BWNuz8Ueby9J7hKMDwarj0HAKVCeAe7o8h3jgciO+KD3TzZI6Y6HI3UBZwphRNgtq/mdQZzK7jt3FmeILVyFd9l71wZVgmOnuDqwcjkMWobHOe7/X39POmnQnbg3fhr5q2XwXsPWUQlKBT4uZrLh6/B5Qo/F2emw+d3PaZGXHrclu8phPidQsEvRKIo+IVIFAW/EImi4BciURT8QiTKnFKfmW0B8BdotOB2ALvd/Ttm9nUAfwpguHnXr7r7k/N1ZLCX18HLbdoUHB9YwSWe8iTvT/W/Dx6ntpkSl4Ae+FS4zmBPD28lVZngLa1WOpfRPOPS1kSVS0DdXWEZcPMAlz7Xr+dr/9yhcA08ABgd4S3RcsXwqeVFXiMx38FrKx47zteqGklmWbWqP2wo8zUs5Hgrr3yOtxSb3bn6Unwe+mEOfM7QmfB61Kr8/L2UVnT+GoCvuPvLZtYH4CUze7pp+7a7/5eWjyaEuGJopVffGQBnmrcnzOwQgKuW2jEhxNJyWd/5zWwrgDsB7GkOfcnM9pvZo2bGfzYnhLjiaDn4zawXwE8BfNndxwF8F8B1AO5A45PBN8m8XWa218z2LoK/QohFoqXgN7MiGoH/fXd/HADc/Zy7Z+5eB/A9AHeF5rr7bnff6e47F8tpIcTCmTP4rbGF+QiAQ+7+rVnjG2fd7dMADiy+e0KIpaKV3f67AXwOwGtmtq859lUAnzWzO9CQ/44B+MJCHBkY4NlexWJYXhlYxefcP7ia2t4++ja1jYyOUNsn7/0YtTFiEs+JU1wqO/yrX1LbzX0nqW3vubDc9OsKlxw/0TdEbf/8w7yt1f94gctK5RXhPeFb77qJzil2canvnXeOUVs90qKKyYAdRIqci1jmXiyjLnYe2Iqw/Fmt8OzNQkf48S5HUmxlt/85hEsMzlvTF0IsP/qFnxCJouAXIlEU/EIkioJfiERR8AuRKFdMAc8bb/l9anv7zfCPA8fHuSy36aoN1HbttduozSKFM6enwnJZrTY/+WflIJcqh6u8qObhI6QnFwD0hbPYJsf4+3yucx215Tu5jDZd5lKUd4YLZ05O8GzFrMZlqmq0hRaf19EZzrh054/nEekwljUXk9lyucu/zuYjRT8rlbAf9cuQ+nTlFyJRFPxCJIqCX4hEUfALkSgKfiESRcEvRKJcMVJfDJbdtCqS1dfVxQtFlma4XFOa5jJatRqWh+LZXNSE7h5u27HzRmobGhmL2ML+X7OFZ9OduDBBbd7Bn9vwNPejuzNc+LNrJvJ4Q8PUFpPzYtJcrhAuxlmtcj+qJV60tEYkNgDYtCVcaBYAurt4kdd8IZy1msvz8CwRmZU9VvDxW76nEOJ3CgW/EImi4BciURT8QiSKgl+IRFHwC5Eo7wupr9gRznCrVHi/tclJ3puuEslGyyJZW1k2j35rFpFeIj33rrmaN0W6976PU9upY+HinqvXD9A5z//yRWp78dARahua5NeOLb3hdRy7EOlduGoltQ2sJT33AKxezYu1MukrH+mrlxl/nQt9XEK++eZbqK1e51mEuXzYx0IH7xnYS7I3f/L439A5/+C4Ld9TCPE7hYJfiERR8AuRKAp+IRJFwS9Eosy5229mXQCeBdDZvP9P3P1rZrYNwA8BrAbwEoDPuTvfKl8AIxcuBMcnp3hiSS7Hd0qLeV4fr0gSQQBggtSfs8jOcT6SnNGR40pAV4G3ydq8uYva1g2E/c918cerZdyPPXv4c1sVuXYM9IePNz15kc754kMPUltM2YnVzpucDCc6TU5GkpkyrgbVs3BtQgD4whe/Qm1XIq1c+csAPu7ut6PRjvs+M/sQgD8D8G13vx7AKIDPL52bQojFZs7g9wbvvn0Wm/8cwMcB/KQ5/hiAB5bEQyHEktDSd34zyzc79A4BeBrA2wDG3P3dz0AnAfBfpQghrjhaCn53z9z9DgCbAdwFgFeGuAQz22Vme80sXHxfCLEsXNZuv7uPAfgFgA8DGDCzd3ezNgM4Rebsdved7r5zQZ4KIRaVOYPfzNaa2UDzdjeAPwJwCI03gX/WvNtDAH6+VE4KIRafVhJ7NgJ4zMzyaLxZ/Njd/8bMDgL4oZn9JwCvAHhkqZyskVZNxSKX7Do6Oqmtp5snkMSSfmpOEjAibZWK3bx2W3fEVihyyTE3M0VtXg9LbFu33UDnXHst/xb3wQ9+iNrqdS57MYmzFKmP9+EP301tYvGZM/jdfT+AOwPjR9H4/i+EeB+iX/gJkSgKfiESRcEvRKIo+IVIFAW/EIlisYyoRT+Y2TCAd5p/rgFwvm0H58iP9yI/3sv7zY9r3H1tKw/Y1uB/z4HN9l4Jv/qTH/IjVT/0sV+IRFHwC5Eoyxn8u5fx2LORH+9FfryX31k/lu07vxBiedHHfiESZVmC38zuM7PDZnbEzB5eDh+afhwzs9fMbF87i42Y2aNmNmRmB2aNrTKzp83sreb/g8vkx9fN7FRzTfaZ2afa4McWM/uFmR00s9fN7N82x9u6JhE/2romZtZlZr8xs1ebfvzH5vg2M9vTjJsfmRlPa20Fd2/rPwB5NMqAXQugA8CrAHa024+mL8cArFmG434EwAcAHJg19p8BPNy8/TCAP1smP74O4N+1eT02AvhA83YfgDcB7Gj3mkT8aOuaADAAvc3bRQB7AHwIwI8BfKY5/l8B/OuFHGc5rvx3ATji7ke9Uer7hwDuXwY/lg13fxbAyCXD96NRCBVoU0FU4kfbcfcz7v5y8/YEGsVirkKb1yTiR1vxBkteNHc5gv8qACdm/b2cxT8dwFNm9pKZ7VomH95lvbufad4+C2D9MvryJTPb3/xasORfP2ZjZlvRqB+xB8u4Jpf4AbR5TdpRNDf1Db973P0DAD4J4Itm9pHldghovPOj8ca0HHwXwHVo9Gg4A+Cb7TqwmfUC+CmAL7v7ezqktHNNAn60fU18AUVzW2U5gv8UgC2z/qbFP5cadz/V/H8IwM+wvJWJzpnZRgBo/j+0HE64+7nmiVcH8D20aU3MrIhGwH3f3R9vDrd9TUJ+LNeaNI992UVzW2U5gv9FANubO5cdAD4D4Il2O2FmPWbW9+5tAJ8AcCA+a0l5Ao1CqMAyFkR9N9iafBptWBNr9Dt7BMAhd//WLFNb14T50e41aVvR3HbtYF6ym/kpNHZS3wbw75fJh2vRUBpeBfB6O/0A8AM0Pj5W0fju9nk0eh4+A+AtAP8LwKpl8uMvAbwGYD8awbexDX7cg8ZH+v0A9jX/fardaxLxo61rAuA2NIri7kfjjeY/zDpnfwPgCID/DqBzIcfRL/yESJTUN/yESBYFvxCJouAXIlEU/EIkioJfiERR8AuRKAp+IRJFwS9EovxfY4Eqrem6/5gAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(image):\n",
    "    img = image.numpy()\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, Linear, MaxPool2d, Dropout, Dropout2d, BatchNorm2d, BatchNorm1d\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "The entire model consists of 14 layers in total. In addition to layers below lists what techniques are applied to build the model.\n",
    "\n",
    "![vggnet.jpg](./images/vggnet.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.conv1 = Conv2d(3, 64, 3, padding=1)\n",
    "        self.conv1_bn = BatchNorm2d(64)\n",
    "        \n",
    "        self.conv2 = Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2_bn = BatchNorm2d(128)\n",
    "        \n",
    "        self.conv3 = Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv4 = Conv2d(256, 256, 3, padding=1)\n",
    "        self.conv4_bn = BatchNorm2d(256)\n",
    "        \n",
    "        self.conv5 = Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv6 = Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv6_bn = BatchNorm2d(512)\n",
    "        \n",
    "        self.conv7 = Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv8 = Conv2d(512, 512, 3, padding=1)\n",
    "        self.conv8_bn = BatchNorm2d(512)\n",
    "        \n",
    "        self.pool = MaxPool2d(2, stride=2)\n",
    "        \n",
    "        self.dropout_2d = Dropout2d(0.25)\n",
    "        \n",
    "        self.dropout_1d = Dropout(0.25)\n",
    "        \n",
    "        self.fc1 = Linear(512 * 1 * 1, 4096)\n",
    "        self.fc1_bn = BatchNorm1d(4096)\n",
    "        \n",
    "        self.fc2 = Linear(4096, 4096)\n",
    "        self.fc2_bn = BatchNorm1d(4096)\n",
    "        \n",
    "        self.fc3 = Linear(4096, 1000)\n",
    "        self.fc3_bn = BatchNorm1d(1000)\n",
    "        \n",
    "        self.fc4 = Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dropout_2d(F.relu(self.conv1(x)))\n",
    "        x = self.conv1_bn(self.pool(x))\n",
    "        \n",
    "        x = self.dropout_2d(F.relu(self.conv2(x)))\n",
    "        x = self.conv2_bn(self.pool(x))\n",
    "        \n",
    "        x = self.dropout_2d(F.relu(self.conv3(x)))\n",
    "        x = self.dropout_2d(F.relu(self.conv4(x)))\n",
    "        x = self.conv4_bn(self.pool(x))\n",
    "        \n",
    "        x = self.dropout_2d(F.relu(self.conv5(x)))\n",
    "        x = self.dropout_2d(F.relu(self.conv6(x)))\n",
    "        x = self.conv6_bn(self.pool(x))\n",
    "        \n",
    "        x = self.dropout_2d(F.relu(self.conv7(x)))\n",
    "        x = self.dropout_2d(F.relu(self.conv8(x)))\n",
    "        x = self.conv8_bn(self.pool(x))\n",
    "        \n",
    "        x = x.view(-1, 1 * 1 * 512)\n",
    "        \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc1_bn(self.dropout_1d(x))\n",
    "        \n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc2_bn(self.dropout_1d(x))\n",
    "        \n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc3_bn(self.dropout_1d(x))\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2_bn): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4_bn): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv5): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv6_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv7): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv8_bn): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dropout_2d): Dropout2d(p=0.25)\n",
      "  (dropout_1d): Dropout(p=0.25)\n",
      "  (fc1): Linear(in_features=512, out_features=4096, bias=True)\n",
      "  (fc1_bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (fc2_bn): BatchNorm1d(4096, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc3): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  (fc3_bn): BatchNorm1d(1000, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc4): Linear(in_features=1000, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "if is_gpu_available:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.517203 \tValidation Loss: 0.315081\n",
      "Validation loss decreased (inf --> 0.315081).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.320906 \tValidation Loss: 0.284388\n",
      "Validation loss decreased (0.315081 --> 0.284388).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.222106 \tValidation Loss: 0.256347\n",
      "Validation loss decreased (0.284388 --> 0.256347).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.141128 \tValidation Loss: 0.234619\n",
      "Validation loss decreased (0.256347 --> 0.234619).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.081302 \tValidation Loss: 0.224059\n",
      "Validation loss decreased (0.234619 --> 0.224059).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.023276 \tValidation Loss: 0.205529\n",
      "Validation loss decreased (0.224059 --> 0.205529).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 0.973108 \tValidation Loss: 0.195605\n",
      "Validation loss decreased (0.205529 --> 0.195605).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.932711 \tValidation Loss: 0.184554\n",
      "Validation loss decreased (0.195605 --> 0.184554).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.892866 \tValidation Loss: 0.181664\n",
      "Validation loss decreased (0.184554 --> 0.181664).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.851917 \tValidation Loss: 0.168199\n",
      "Validation loss decreased (0.181664 --> 0.168199).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.829236 \tValidation Loss: 0.165806\n",
      "Validation loss decreased (0.168199 --> 0.165806).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.796447 \tValidation Loss: 0.163514\n",
      "Validation loss decreased (0.165806 --> 0.163514).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.774073 \tValidation Loss: 0.152578\n",
      "Validation loss decreased (0.163514 --> 0.152578).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.751104 \tValidation Loss: 0.145073\n",
      "Validation loss decreased (0.152578 --> 0.145073).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.731847 \tValidation Loss: 0.143045\n",
      "Validation loss decreased (0.145073 --> 0.143045).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.708428 \tValidation Loss: 0.142679\n",
      "Validation loss decreased (0.143045 --> 0.142679).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.697109 \tValidation Loss: 0.137965\n",
      "Validation loss decreased (0.142679 --> 0.137965).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.673789 \tValidation Loss: 0.132661\n",
      "Validation loss decreased (0.137965 --> 0.132661).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.655923 \tValidation Loss: 0.129080\n",
      "Validation loss decreased (0.132661 --> 0.129080).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.649141 \tValidation Loss: 0.128088\n",
      "Validation loss decreased (0.129080 --> 0.128088).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.633228 \tValidation Loss: 0.125436\n",
      "Validation loss decreased (0.128088 --> 0.125436).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.624278 \tValidation Loss: 0.125525\n",
      "Epoch: 23 \tTraining Loss: 0.606145 \tValidation Loss: 0.121988\n",
      "Validation loss decreased (0.125436 --> 0.121988).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.591160 \tValidation Loss: 0.122204\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 30\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        if is_gpu_available:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        if is_gpu_available:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    total_train_loss.append(train_loss)\n",
    "    total_valid_loss.append(valid_loss)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=x_test, y=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model.h5')\n",
    "history = model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    " \n",
    "### Training from scratch\n",
    "\n",
    "Training AlexNet, using stochastic gradient descent with a fixed learning rate of 0.01, for 50 epochs, we acheive a test accuracy of ~76.75%.\n",
    "\n",
    "In accuracy and loss plot shown below, notice the large gap between the training and testing curves. This suggests that our model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "- Expriment to stop model overfiting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
