{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet (Inception v1)\n",
    "GoogLeNet won the LSVRC-2014 competition. It has significant improvement over ZFNet (The winner in 2013) and AlexNet (The winner in 2012) and has relatively lower error rate compared with the VGGNet(1st runner-up in 2014).\n",
    "\n",
    "### The highlights of the paper\n",
    "- It contains 1×1 Convolution at the middle of the network. 1×1 convolution is used as a dimension reduction module to reduce the computation. By reducing the computation bottleneck, depth and width can be increased.\n",
    "- Global average pooling is used at the end of the network instead of using fully connected layers. Global Average can be less prone to overfitting\n",
    "- Inception module, is to have different sizes/types of convolutions for the same input and stacking all the outputs.\n",
    "- Local Response Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The architecture\n",
    "#### Global average pooling\n",
    "![gap_layer.png](./images/gap_layer.png)\n",
    "Global average pooling (GAP) layers to minimize overfitting by reducing the total number of parameters in the model. Similar to max pooling layers, GAP layers are used to reduce the spatial dimensions of a three-dimensional tensor. However, GAP layers perform a more extreme type of dimensionality reduction, where a tensor with dimensions h×w×d is reduced in size to have dimensions 1×1×d. GAP layers reduce each h×w feature map to a single number by simply taking the average of all hw values.\n",
    "\n",
    "#### Local Response Normalization\n",
    "This layer is useful when we are dealing with ReLU neurons. Why is that? Because ReLU neurons have unbounded activations and we need LRN to normalize that. We want to detect high frequency features with a large response. If we normalize around the local neighborhood of the excited neuron, it becomes even more sensitive as compared to its neighbors. At the same time, it will dampen the responses that are uniformly large in any given local neighborhood. If all the values are large, then normalizing those values will diminish all of them. So basically we want to encourage some kind of inhibition and boost the neurons with relatively larger activations. \n",
    "\n",
    "#### Inception Module\n",
    "<div width=\"100%\">\n",
    "    <img src=\"./images/inception_naive.png\" width=\"40%\">\n",
    "    <img src=\"./images/inception_reduction.png\" width=\"40%\">\n",
    "<div>\n",
    "The intention is to let the neural network learn the best weights when training the network and automatically select the more useful features. Additionally, it intends to reduce the no. of dimensions so that the no. of units and layers can be increased at later stages. The side-effect of this is to increase the computational cost for training this layer. To address this, a number of solutions have been suggested in the paper such as to deploy parallel computations for this architecture.\n",
    "\n",
    "#### Overall Architecture\n",
    "GoogLeNet has 9 such inception modules stacked linearly. It is 22 layers deep (27, including the pooling layers). It uses global average pooling at the end of the last inception module. To prevent the middle part of the network from “dying out”, the authors introduced two auxiliary classifiers. They essentially applied softmax to the outputs of two of the inception modules, and computed an auxiliary loss over the same labels. The total loss function is a weighted sum of the auxiliary loss and the real loss.\n",
    "\n",
    "![orignal_architech.png](./images/original_architecture.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GoogLeNet implementation on Cifar 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on Gpu\n"
     ]
    }
   ],
   "source": [
    "is_gpu_available = torch.cuda.is_available()\n",
    "if is_gpu_available:\n",
    "    print(\"Training on Gpu\")\n",
    "else:\n",
    "    print(\"Training on Cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR10 small image classification\n",
    "Dataset of 50,000 32x32 color training images, labeled over 10 categories, and 10,000 test images.\n",
    "\n",
    "#### Returns 2 tuples:\n",
    "- **x_train, x_test**: uint8 array of RGB image data with shape (num_samples, 32, 32, 3).\n",
    "- **y_train, y_test**: uint8 array of category labels (integers in range 0-9) with shape (num_samples,)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Number of training examples = 50000\n",
      "Number of testing examples = 10000\n",
      "Image data shape = torch.Size([3, 32, 32])\n",
      "Number of classes = 10\n"
     ]
    }
   ],
   "source": [
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision.transforms import Compose, RandomHorizontalFlip, RandomRotation, ToTensor, Normalize\n",
    "\n",
    "transform = Compose([\n",
    "    RandomHorizontalFlip(),\n",
    "    RandomRotation(10),\n",
    "    ToTensor(),\n",
    "    Normalize((0.5, 0.5, 0.5),(0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "train_data = CIFAR10('data', train=True, transform=transform, download=True)\n",
    "test_data = CIFAR10('data', train=False, transform=transform, download=True)\n",
    "\n",
    "# specify the image classes\n",
    "classes = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "           'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = 10\n",
    "print(\"Number of training examples =\", len(train_data))\n",
    "print(\"Number of testing examples =\", len(test_data))\n",
    "print(\"Image data shape =\", train_data[0][0].shape)\n",
    "print(\"Number of classes =\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "train_length = len(train_data)\n",
    "valid_size = np.int(np.floor(0.2 * train_length))\n",
    "indcies = list(range(0, train_length))\n",
    "np.random.shuffle(indcies)\n",
    "train_idx, valid_idx = indcies[valid_size:], indcies[:valid_size]\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=10, sampler=train_sampler)\n",
    "test_loader = DataLoader(test_data, batch_size=10)\n",
    "valid_loader = DataLoader(train_data, batch_size=10, sampler=valid_sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image):\n",
    "    img = image.numpy()\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "imshow(images[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, Linear, MaxPool2d, BatchNorm2d, BatchNorm1d, Dropout, Dropout2d\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "The entire model consists of 19 inception layers in total.\n",
    "![inception_parameters.png](./images/inception_parameters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_keys = {\n",
    "    \"3a\" : {\n",
    "        \"input\": 192,\n",
    "        \"1_conv\": 64,\n",
    "        \"3_convr\": 96,\n",
    "        \"3_conv\": 128,\n",
    "        \"5_convr\": 16,\n",
    "        \"5_conv\": 32,\n",
    "        \"pool_p\": 32\n",
    "    },\n",
    "    \"3b\" : {\n",
    "        \"input\": 256,\n",
    "        \"1_conv\": 128,\n",
    "        \"3_convr\": 128,\n",
    "        \"3_conv\": 192,\n",
    "        \"5_convr\": 32,\n",
    "        \"5_conv\": 96,\n",
    "        \"pool_p\": 64\n",
    "    },\n",
    "    \"4a\" : {\n",
    "        \"input\": 480,\n",
    "        \"1_conv\": 192,\n",
    "        \"3_convr\": 96,\n",
    "        \"3_conv\": 208,\n",
    "        \"5_convr\": 16,\n",
    "        \"5_conv\": 48,\n",
    "        \"pool_p\": 64\n",
    "    },\n",
    "    \"4b\" : {\n",
    "        \"input\": 512,\n",
    "        \"1_conv\": 160,\n",
    "        \"3_convr\": 112,\n",
    "        \"3_conv\": 224,\n",
    "        \"5_convr\": 24,\n",
    "        \"5_conv\": 64,\n",
    "        \"pool_p\": 64,\n",
    "    },\n",
    "    \"4c\" : {\n",
    "        \"input\": 512,\n",
    "        \"1_conv\": 128,\n",
    "        \"3_convr\": 128,\n",
    "        \"3_conv\": 256,\n",
    "        \"5_convr\": 24,\n",
    "        \"5_conv\": 64,\n",
    "        \"pool_p\": 64\n",
    "    },\n",
    "    \"4d\" : {\n",
    "        \"input\": 512,\n",
    "        \"1_conv\": 112,\n",
    "        \"3_convr\": 144,\n",
    "        \"3_conv\": 288,\n",
    "        \"5_convr\": 32,\n",
    "        \"5_conv\": 64,\n",
    "        \"pool_p\": 64,\n",
    "    },\n",
    "    \"4e\" : {\n",
    "        \"input\": 528,\n",
    "        \"1_conv\": 256,\n",
    "        \"3_convr\": 160,\n",
    "        \"3_conv\": 320,\n",
    "        \"5_convr\": 32,\n",
    "        \"5_conv\": 128,\n",
    "        \"pool_p\": 128\n",
    "    },\n",
    "    \"5a\" : {\n",
    "        \"input\": 832,\n",
    "        \"1_conv\": 256,\n",
    "        \"3_convr\": 160,\n",
    "        \"3_conv\": 320,\n",
    "        \"5_convr\": 32,\n",
    "        \"5_conv\": 128,\n",
    "        \"pool_p\": 128\n",
    "    },\n",
    "    \"5b\" : {\n",
    "        \"input\": 832,\n",
    "        \"1_conv\": 384,\n",
    "        \"3_convr\": 192,\n",
    "        \"3_conv\": 384,\n",
    "        \"5_convr\": 48,\n",
    "        \"5_conv\": 128,\n",
    "        \"pool_p\": 128\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Inception(Module):\n",
    "    def __init__(self, keys):\n",
    "        super(Inception, self).__init__()\n",
    "        self.conv1 = Conv2d(keys['input'], keys['1_conv'], 1)\n",
    "        \n",
    "        self.conv3r = Conv2d(keys['input'], keys['3_convr'], 1)\n",
    "        self.conv3 = Conv2d(keys['3_convr'], keys['3_conv'], 3, padding=1)\n",
    "        \n",
    "        self.conv5r = Conv2d(keys['input'], keys['5_convr'], 1)\n",
    "        self.conv5 = Conv2d(keys['5_convr'], keys['5_conv'], 5, padding=2)\n",
    "        \n",
    "        self.pool = MaxPool2d(3,padding=1, stride=1)\n",
    "        self.conv_p = Conv2d(keys['input'], keys['pool_p'], 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        cn1 = F.relu(self.conv1(x))\n",
    "        \n",
    "        cn3 = F.relu(self.conv3r(x))\n",
    "        cn3 = F.relu(self.conv3(cn3))\n",
    "        \n",
    "        cn5 = F.relu(self.conv5r(x))\n",
    "        cn5 = F.relu(self.conv5(cn5))\n",
    "        \n",
    "        p1 = self.pool(x)\n",
    "        p1 = self.conv_p(p1)\n",
    "        \n",
    "        output = [cn1, cn3, cn5, p1]\n",
    "        \n",
    "        return torch.cat(output,1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Flatten(Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Flatten, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        shape = torch.prod(torch.tensor(x.shape[1:])).item()\n",
    "        return x.view(-1, shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = Conv2d(in_channels=3, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1_bn = BatchNorm2d(64)\n",
    "        self.conv2 = Conv2d(in_channels=64, out_channels=64, kernel_size=1)\n",
    "        self.conv3 = Conv2d(in_channels=64, out_channels=192, kernel_size=3, padding=1)\n",
    "        self.conv3_bn = BatchNorm2d(192)\n",
    "        self.inception_3a = Inception(inception_keys['3a'])\n",
    "        self.inception_3b = Inception(inception_keys['3b'])\n",
    "        self.inception_4a = Inception(inception_keys['4a'])\n",
    "        self.inception_4b = Inception(inception_keys['4b'])\n",
    "        self.inception_4c = Inception(inception_keys['4c'])\n",
    "        self.inception_4d = Inception(inception_keys['4d'])\n",
    "        self.inception_4e = Inception(inception_keys['4e'])\n",
    "        self.inception_5a = Inception(inception_keys['5a'])\n",
    "        self.inception_5b = Inception(inception_keys['5b'])\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        \n",
    "        self.fc1 = Linear(1024, 1000)\n",
    "        self.fc2 = Linear(1000, 10)\n",
    "        \n",
    "        self.dropout = Dropout(0.4)\n",
    "        self.pool = MaxPool2d(kernel_size=3, padding=1, stride=2)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.conv1_bn(self.pool(x))\n",
    "        \n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.conv3_bn(self.pool(x))\n",
    "        \n",
    "        x = self.inception_3a(x)\n",
    "        x = self.inception_3b(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.inception_4a(x)\n",
    "        x = self.inception_4b(x)\n",
    "        x = self.inception_4c(x)\n",
    "        x = self.inception_4d(x)\n",
    "        x = self.inception_4e(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.inception_5a(x)\n",
    "        x = self.inception_5b(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv1_bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  (conv3): Conv2d(64, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv3_bn): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (inception_3a): Inception(\n",
      "    (conv1): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(192, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(96, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(192, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(192, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_3b): Inception(\n",
      "    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(128, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(256, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(32, 96, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_4a): Inception(\n",
      "    (conv1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(480, 96, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(96, 208, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(480, 16, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(16, 48, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(480, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_4b): Inception(\n",
      "    (conv1): Conv2d(512, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(112, 224, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_4c): Inception(\n",
      "    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(512, 24, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(24, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_4d): Inception(\n",
      "    (conv1): Conv2d(512, 112, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(512, 144, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(144, 288, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(512, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_4e): Inception(\n",
      "    (conv1): Conv2d(528, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(528, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(528, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(528, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_5a): Inception(\n",
      "    (conv1): Conv2d(832, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(832, 160, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(160, 320, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(832, 32, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(32, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (inception_5b): Inception(\n",
      "    (conv1): Conv2d(832, 384, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3r): Conv2d(832, 192, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv3): Conv2d(192, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (conv5r): Conv2d(832, 48, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (conv5): Conv2d(48, 128, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=1, padding=1, dilation=1, ceil_mode=False)\n",
      "    (conv_p): Conv2d(832, 128, kernel_size=(1, 1), stride=(1, 1))\n",
      "  )\n",
      "  (flatten): Flatten()\n",
      "  (fc1): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  (fc2): Linear(in_features=1000, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.4)\n",
      "  (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a complete CNN\n",
    "model = Net()\n",
    "print(model)\n",
    "\n",
    "if is_gpu_available:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "import torch.optim as optim\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = CrossEntropyLoss()\n",
    "\n",
    "# specify optimizer\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.842437 \tValidation Loss: 0.460609\n",
      "Validation loss decreased (inf --> 0.460609).  Saving model ...\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 1\n",
    "\n",
    "valid_loss_min = np.Inf # track change in validation loss\n",
    "total_train_loss = []\n",
    "total_valid_loss = []\n",
    "\n",
    "for epoch in range(1, n_epochs+1):\n",
    "\n",
    "    # keep track of training and validation loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model.train()\n",
    "    for data, target in train_loader:\n",
    "        # clear the gradients of all optimized variables\n",
    "        if is_gpu_available:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer.step()\n",
    "        # update training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model.eval()\n",
    "    for data, target in valid_loader:\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        if is_gpu_available:\n",
    "            data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        # calculate the batch loss\n",
    "        loss = criterion(output, target)\n",
    "        # update average validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    # calculate average losses\n",
    "    train_loss = train_loss/len(train_loader.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader.dataset)\n",
    "    \n",
    "    total_train_loss.append(train_loss)\n",
    "    total_valid_loss.append(valid_loss)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch, train_loss, valid_loss))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model.state_dict(), 'model_cifar.pt')\n",
    "        valid_loss_min = valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"neutron.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x=x_test, y=[y_test, y_test, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('inception_v1_model.h5')\n",
    "history = model.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    " \n",
    "### Training from scratch\n",
    "\n",
    "Training AlexNet, using stochastic gradient descent with a fixed learning rate of 0.01, for 50 epochs, we acheive a test accuracy of ~76.75%.\n",
    "\n",
    "In accuracy and loss plot shown below, notice the large gap between the training and testing curves. This suggests that our model is overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in history.history.keys():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['activation_acc'])\n",
    "plt.plot(history.history['activation_1_acc'])\n",
    "plt.plot(history.history['activation_2_acc'])\n",
    "plt.plot(history.history['val_activation_acc'])\n",
    "plt.plot(history.history['val_activation_1_acc'])\n",
    "plt.plot(history.history['val_activation_2_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train_S1', 'Train_S2', 'Train_S3', 'Test_S1', 'Test_S2', 'Test_S3'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "- Expriment to stop model overfiting"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
